# This file is editable by non-programmers, to set how the tool will run.
# Lines beginning with # are ignored.

---

General:
  # This section has some general configurations

  # The minimum logging messages that will appear on screen
  # Choose from: DEBUG, INFO, WARNING, ERROR, CRITICAL
  # "Console" will log to the screen, and "File" will log to a .log file.
  ConsoleLoggingLevel: INFO
  FileLoggingLevel: DEBUG


LLM:
  # This section has to do with the LLM model used and the parameters set to the model.

  # The LLM to actually use
  # Available choices are:
  #     Claude3Haiku
  #     Llama213bChat
  #     Llama270bChat
  #     Mistral7bInstruct
  #     Mixtral8x7bInstruct
  LLMName: Claude3Haiku
  # Lower temperature to make the models not go crazy
  Temperature: 0
  # Lower nucleus sampling (top_p) to not get varying results
  NucleusSampling: 0
  # Top tokens to consider when selecting next token (top_k). Llama 2 doesn't use this.
  TopTokens: 1
  # Maximum tokens in the output
  MaxGenerationLength: 2000


Prompt Engineering:
  # This section includes the text used in the prompts that feed in the model.

  # Some variables are supported in some prompts, such as Text, Text_Context (e.g. page 1), Field_Label (e.g. Authors), Field_Description, etc.
  # Variables need to be enclosed in {}. Check out the existing prompts for examples.

  # You can also do follow-up prompts, ex:
  # PromptLiterature1 = What's the answer to X?
  # PromptLiterature2 = Now make sure X is in the right units.
  # PromptLiterature3 = Just isolate the number in the answer.
  # You can have just one prompt, or multiple follow-up prompts.

  # This is included only at the very start of the prompt chain (need to encase in quotes to not interpret the square brackets as a list)
  Prelude: '[The following is a question and answer pair to extract information from a clinical trial paper. Respond only with accurate data that is found in the document provided in the paper. Be precise, succinct, and accurate.]'
  # This is included at the start of every question (not necessarily needed) (two single quotes is an empty string)
  Prefix: '[Extract ONLY from the clinical trial page(s) above. Respond with just the answer without any additional words or explanations, and only retrieve the sure values found on the page!]'
  # This is the role of the LLM (this is not used in User-Assistant models like Claude 3)
  HectreRole: Answer
  # This is the role of the user (this is not used in User-Assistant models like Claude 3)
  UserRole: Question

  # Prompts for extracting literature data
  PromptLiterature1: |-
    Below is {Text_Context} from a clinical trial paper:
    {Text_Start_Indicator}{Text}{Text_End_Indicator}
    I want to find data in JSON format; here is a template JSON to fill in (enclose values in quotes):
    {Template}
    If any fields cannot be extracted, fill in that entry with "{No_Data}". Please respond with just the JSON.
  #PromptLiterature2: |-
  #  Ok, now tell me just the answer with no other words, or with "{No_Data}" if it cannot be found.

  # Prompts for extracting treatment arms
  PromptTreatmentArms1: |-
    Below is {Text_Context} from a clinical trial paper:
    {Text_Start_Indicator}{Text}{Text_End_Indicator}
    Please identify and list all the treatment arms mentioned in the paper. Treatment arms are typically described as interventions, drugs, doses, or combinations thereof. Respond with just the treatment arms, separated by semi-colons, with no other words. If the treatment arms cannot be deduced from the context, please respond with "{No_Data}".

  # Prompts for extracting data per treatment arm
  PromptPerTreatmentArm1: |-
    Below is {Text_Context} from a clinical trial paper:
    {Text_Start_Indicator}{Text}{Text_End_Indicator}
    Find data in JSON for {Treatment_Arm}; here is a template JSON to fill in (enclose values in quotes):
    {Template}
    If any fields cannot be found, fill in that entry with "{No_Data}". Please respond with just the JSON.

  # Prompts for extracting the nominal time values
  PromptTimeValues1: |-
    Below is {Text_Context} from a clinical trial paper:
    {Text_Start_Indicator}{Text}{Text_End_Indicator}
    I want to find all the time values that the clinical trial measures at in the paper that has some baseline/response/change data. Please respond with just the time values each with units, with each separated by semi-colon with no other words, or with "{No_Data}" if they cannot be deduced from this page.

  # Prompts for extracting all the unique stat analysis groups
  PromptStatGroups1: |-
    Below is {Text_Context} from a clinical trial paper:
    {Text_Start_Indicator}{Text}{Text_End_Indicator}
    Find all of the unique statistical analysis groups for the outcome {Outcome}, in the form of a list of dictionaries; here is a template JSON to fill in for one element in the output list (enclose values in quotes):
    {Template}
    If any fields cannot be found, fill in that entry with "{No_Data}". Please respond with just the list of dictionaries JSON with no other words.

  # Ask the LLM about the type of this outcome
  PromptOutcomeType1: |-
    For the outcome endpoint {Outcome}, if this is a binary endpoint (either a population is or isn't) respond with 1, if it is a continuous endpoint (has value that changes over time) respond with 2, and if neither then respond with 0. Reply with only the answer with no other punctuations or texts.

  # Prompts for extracting clinical data information
  PromptClinical1: |-
    Below is {Text_Context} from a clinical trial paper:
    {Text_Start_Indicator}{Text}{Text_End_Indicator}
    I want to find some data in JSON for {Treatment_Arm} at time {Time_Value} for endpoint {Outcome_Type}{Outcome}, with {Stat_Group}. Adhere strictly to these treatment arms, time, endpoints, and statistical analysis! Here is a template JSON to fill in (enclose values in quotes):
    {Template}
    If any fields cannot be found, fill in that entry with "{No_Data}". Please respond with just the JSON with no other words.
